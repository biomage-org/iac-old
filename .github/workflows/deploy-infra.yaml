name: Deploy Biomage infrastructure on AWS
on:
  workflow_dispatch:
    inputs:
      environment_name:
        type: choice
        description: Select the environment name to run the actions on
        options:
          - trv
          - Biomage
          - all
        default: all
      workflow_actions:
        type: choice
        description: Select actions to perform
        options:
        - deploy and configure
        - configure
        default: configure
      cluster:
        type: choice
        description: Select cluster
        options:
        - staging
        - production
        - staging and production
        default: staging
env:
  RELEASES_REPOSITORY: $GITHUB_REPOSITORY_OWNER/releases

# this ensures that only one CI pipeline with the same key
#  can run at once in order to prevent undefined states
concurrency: cluster-update-mutex

jobs:
  set-environments:
    name: Set up environment name and environment type for action run
    runs-on: ubuntu-20.04
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      environment_name: ${{ steps.set-environment-name.outputs.environment_name }}
    steps:
      - id: set-matrix
        name: Set up cluster matrix
        run: |-
          if [ "${CLUSTER}" = "staging" ]; then
            echo 'matrix=["staging"]' >> $GITHUB_OUTPUT
          elif [ "${CLUSTER}" = "production" ]; then
            echo 'matrix=["production"]' >> $GITHUB_OUTPUT
          elif [ "${CLUSTER}" = "staging and production" ]; then
            echo 'matrix=["staging", "production"]' >> $GITHUB_OUTPUT
          fi
        env:
          CLUSTER: ${{ github.event.inputs.cluster }}
      - id: set-environment-name
        name: Set up environment name
        run: |-
          if [ "${ENVIRONMENT_NAME}" = "all" ]; then
            echo 'environment_name=["Biomage", "trv"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "trv" ]; then
            echo 'environment_name=["trv"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "Biomage" ]; then
            echo 'environment_name=["Biomage"]' >> $GITHUB_OUTPUT
          fi
        env:
          ENVIRONMENT_NAME: ${{ github.event.inputs.environment_name }}


  create-eks-cluster:
    name: Create EKS cluster
    runs-on: ubuntu-20.04
    needs: set-environments
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
    strategy:
      max-parallel: 1
      matrix:
        environment-type: ${{ fromJson(needs.set-environments.outputs.matrix) }}
        environment-name: ${{ fromJson(needs.set-environments.outputs.environment_name) }}
        exclude:
          - environment-name: trv
            environment-type: staging
    environment: ${{ matrix.environment-name }}
    if: github.event.inputs.workflow_actions == 'deploy and configure'
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      # - id: fill-metadata
      #   name: Add name and region to the eksctl file.
      #   run: |-
      #     export CLUSTER_NAME="biomage-$CLUSTER_ENV"
      #     yq -i '
      #       .metadata.name = strenv(CLUSTER_NAME) |
      #       .metadata.region = strenv(AWS_REGION)
      #     ' infra/config/cluster/cluster-template.yaml
          
      #     export CUSTOM_CLUSTER_CONFIG="infra/config/cluster/${{ matrix.environment-name }}/custom-cluster-config.yaml"
      #     yq eval-all '. as $item ireduce ({}; . *d $item)' infra/config/cluster/cluster-template.yaml ${CUSTOM_CLUSTER_CONFIG} > /tmp/cluster-$CLUSTER_ENV.yaml
      #     cat /tmp/cluster-$CLUSTER_ENV.yaml
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}

  #     - id: install-eksctl
  #       name: Install eksctl
  #       run: |-
  #         curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
  #         sudo mv /tmp/eksctl /usr/local/bin

  #     - id: create-clusters
  #       name: Attempt to create clusters from spec.
  #       # this job will always pass, irrespective of whether creation was successful or not.
  #       # this is because the cluster may already exist. we will check for this condition
  #       # on failure in the next step
  #       continue-on-error: true
  #       run: |-
  #         exec &> >(tee /tmp/eksctl-$CLUSTER_ENV.log)

  #         eksctl create cluster -f /tmp/cluster-$CLUSTER_ENV.yaml
  #         echo "::set-output name=outcome::created"
  #     - id: check-for-failure
  #       name: Check for reason of failure if cluster creation failed.
  #       if: steps.create-clusters.outcome == 'failure'
  #       run: |-
  #         # Check if failure was caused by an already exists exception.
  #         # If not, the job should fail.
  #         ALREADY_EXISTS=$(grep AlreadyExistsException /tmp/eksctl-$CLUSTER_ENV.log | wc -l | xargs)
  #         if [ $ALREADY_EXISTS -ne 1 ]
  #         then
  #           echo Step failed for reason other than stack already existing.
  #           echo Job failing...
  #           echo "::set-output name=reason::error"
  #           false
  #         fi

  #         echo Cluster already exists.
  #         echo "::set-output name=reason::already-exists"

  #     - id: update-nodegroup
  #       name: Attempt to update node groups for existing cluster.
  #       if: steps.create-clusters.outcome == 'failure' && steps.check-for-failure.outputs.reason == 'already-exists'
  #       run: |-
  #         eksctl create nodegroup --config-file=/tmp/cluster-$CLUSTER_ENV.yaml
  #         eksctl delete nodegroup --config-file /tmp/cluster-$CLUSTER_ENV.yaml --only-missing --approve

  #     # note: iam service accounts should really be created from within the helm chart as seen here:
  #     # https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html
  #     - id: update-serviceaccounts
  #       name: Attempt to update IAM service accounts for existing cluster.
  #       if: steps.create-clusters.outcome == 'failure' && steps.check-for-failure.outputs.reason == 'already-exists'
  #       run: |-
  #         eksctl utils associate-iam-oidc-provider --config-file=/tmp/cluster-$CLUSTER_ENV.yaml --approve
  #         eksctl create iamserviceaccount --config-file=/tmp/cluster-$CLUSTER_ENV.yaml
  #         eksctl delete iamserviceaccount --config-file=/tmp/cluster-$CLUSTER_ENV.yaml --only-missing --approve

  configure-cluster:
    name: Configure Kubernetes resources on the EKS cluster
    runs-on: ubuntu-20.04
    needs: [set-environments, create-eks-cluster]
    if: always() && (needs.create-eks-cluster.result == 'success' || needs.create-eks-cluster.result == 'skipped')
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
      API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
    strategy:
      max-parallel: 1
      matrix:
        environment-type: ${{ fromJson(needs.setup.outputs.matrix) }}
        environment-name: ${{ fromJson(needs.set-environments.outputs.environment_name) }}
        exclude:
          - environment-name: trv
            environment-type: staging
    environment: ${{ matrix.environment-name }}
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

  #     - id: add-kubeconfig
  #       name: Add k8s config file for existing cluster.
  #       run: |-
  #         aws eks update-kubeconfig --name biomage-$CLUSTER_ENV

  #     - id: deploy-metrics-server
  #       name: Deploy k8s metrics server
  #       run: |-
  #         kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

  #     - id: create-flux-namespace
  #       name: Attempt to create flux namespace
  #       continue-on-error: true
  #       run: |-
  #         kubectl create namespace flux

  #     - id: setup-domain
  #       name: Compile environment-specific domain name
  #       run: |-
  #         if [ "${{ matrix.environment-type }}" = "production" ]; then
  #           PRIMARY_DOMAIN_NAME="${{ secrets.PRIMARY_DOMAIN_NAME }}"
  #           DOMAIN_NAME="${{ secrets.DOMAIN_NAME }}"
  #         fi
  #         if [ "${{ matrix.environment-type }}" = "staging" ]; then
  #           PRIMARY_DOMAIN_NAME="${{ secrets.PRIMARY_DOMAIN_NAME }}"
  #           DOMAIN_NAME="${{ secrets.DOMAIN_NAME_STAGING }}"
  #         fi
  #         echo "::set-output name=primary-domain-name::$PRIMARY_DOMAIN_NAME"
  #         echo "::set-output name=domain-name::$DOMAIN_NAME"

  #     - id: platform-monitoring-enabled
  #       name: Get config for whether monitoring should be enabled for deployment
  #       uses: mikefarah/yq@master
  #       with:
  #         cmd: yq '.[env(ENVIRONMENT_NAME)].monitoringEnabled' 'infra/config/github-environments-config.yaml'
  #       env:
  #         ENVIRONMENT_NAME: ${{ matrix.environment-name }}

  #     - id: fill-account-specific-metadata
  #       name: Fill in account specific metadata in ConfigMap
  #       run: |-
  #         yq -i '
  #           .myAccount.domainName = strenv(DOMAIN_NAME) |
  #           .myAccount.region = strenv(AWS_REGION) |
  #           .myAccount.accountId = strenv(AWS_ACCOUNT_ID) |
  #           .myAccount.acmCertificate = strenv(ACM_CERTIFICATE_ARN) |
  #           .myAccount.datadogEnabled = strenv(MONITORING_ENABLED) |
  #           .myAccount.datadogApiKey = ""
  #         ' infra/config/account-config.yaml

  #         if [[ $MONITORING_ENABLED = "true" ]]
  #         then
  #           export API_KEY="${{ secrets.DATADOG_API_KEY }}"
  #           yq -i '.myAccount.datadogApiKey = strenv(API_KEY)' infra/config/account-config.yaml
  #         fi

  #         cat infra/config/account-config.yaml

  #       env:
  #         AWS_REGION: ${{ secrets.AWS_REGION }}
  #         AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}
  #         DOMAIN_NAME: ${{ steps.setup-domain.outputs.domain-name }}
  #         ACM_CERTIFICATE_ARN: ${{ secrets.ACM_CERTIFICATE_ARN }}
  #         MONITORING_ENABLED: ${{ steps.platform-monitoring-enabled.outputs.result }}

  #     - id: create-account-information-configmap
  #       name: Create a configmap containing AWS account specific details
  #       continue-on-error: false
  #       run: |-
  #         kubectl create configmap account-config --from-file=infra/config/account-config.yaml -n flux -o yaml --dry-run | kubectl apply -f -

  #     - id: install-fluxctl
  #       name: Install fluxctl
  #       run: |-
  #         sudo snap install fluxctl --classic

  #     - id: install-helm
  #       name: Install Helm
  #       run: |-
  #         sudo snap install helm --classic

  #     - id: install-eksctl
  #       name: Install eksctl
  #       run: |-
  #         curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
  #         sudo mv /tmp/eksctl /usr/local/bin

  #     - id: deploy-load-balancer-role
  #       name: Deploy permissions for AWS load balancer controller
  #       run: |-
  #         curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json

  #         aws iam create-policy \
  #           --policy-name AWSLoadBalancerControllerIAMPolicy-$CLUSTER_ENV \
  #           --policy-document file://iam-policy.json || true

  #         eksctl create iamserviceaccount \
  #           --cluster=biomage-$CLUSTER_ENV \
  #           --namespace=kube-system \
  #           --name=aws-load-balancer-controller \
  #           --attach-policy-arn=arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:policy/AWSLoadBalancerControllerIAMPolicy-$CLUSTER_ENV \
  #           --override-existing-serviceaccounts \
  #           --approve

  #     # we need to retry this due to an active issue with the AWS Load Balancer Controller
  #     # where there are intermittent failures that are only fixable by retrying
  #     # see issue at https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/2071
  #     - id: install-lbc
  #       name: Deploy AWS Load Balancer Controller
  #       uses: nick-invision/retry@v2
  #       with:
  #         timeout_seconds: 600
  #         max_attempts: 20
  #         retry_on: error
  #         on_retry_command: sleep $(shuf -i 5-15 -n 1)
  #         command: |-
  #           helm repo add eks https://aws.github.io/eks-charts
  #           kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"

  #           helm repo update

  #           helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \
  #             --namespace kube-system \
  #             --set serviceAccount.create=false \
  #             --set serviceAccount.name=aws-load-balancer-controller \
  #             --set clusterName=biomage-$CLUSTER_ENV \
  #             --install --wait

  #           # this is needed so SNS does not stop trying to subscribe to not-yet-deployed
  #           # API staging environments because their endpoints are not yet available.
  #           helm upgrade aws-elb-503-subscription-endpoint infra/aws-elb-503-subscription-endpoint \
  #             --namespace default \
  #             --set clusterEnv=$CLUSTER_ENV \
  #             --set acmCertificate=${{ secrets.ACM_CERTIFICATE_ARN }} \
  #             --install --wait

  #     - id: platform-public-facing
  #       name: Get config for whether platform should be public facing
  #       uses: mikefarah/yq@master
  #       with:
  #         cmd: yq '.[env(ENVIRONMENT_NAME)].publicFacing' 'infra/config/github-environments-config.yaml'
  #       env:
  #         ENVIRONMENT_NAME: ${{ matrix.environment-name }}

  #     - id: deploy-env-loadbalancer
  #       name: Deploy AWS Application Load Balancer for environment
  #       uses: aws-actions/aws-cloudformation-github-deploy@v1
  #       with:
  #         parameter-overrides: "Environment=${{ matrix.environment-type }},PublicFacing=${{ steps.platform-public-facing.outputs.result }}"
  #         name: "biomage-k8s-alb-${{ matrix.environment-type }}"
  #         template: 'infra/cf-loadbalancer.yaml'
  #         no-fail-on-empty-changeset: "1"

  #     - id: deploy-route53
  #       name: Deploy Route 53 DNS records to ELB
  #       uses: aws-actions/aws-cloudformation-github-deploy@v1
  #       with:
  #         parameter-overrides: "Environment=${{ matrix.environment-type }},DNSName=${{ steps.deploy-env-loadbalancer.outputs.DNSName }},HostedZoneId=${{ steps.deploy-env-loadbalancer.outputs.CanonicalHostedZoneID }},PrimaryDomainName=${{ steps.setup-domain.outputs.primary-domain-name }},DomainName=${{ steps.setup-domain.outputs.domain-name }}"
  #         name: "biomage-alb-route53-${{ matrix.environment-type }}"
  #         template: 'infra/cf-route53.yaml'
  #         no-fail-on-empty-changeset: "1"

  #     - id: flux-git-read-only
  #       name: Get Flux mode
  #       uses: mikefarah/yq@master
  #       with:
  #         cmd: yq '.[env(ENVIRONMENT_NAME)].fluxGitReadOnly' 'infra/config/github-environments-config.yaml'
  #       env:
  #         ENVIRONMENT_NAME: ${{ matrix.environment-name }}

  #     - id: install-flux
  #       name: Deploy flux
  #       run: |-
  #         helm repo add fluxcd https://charts.fluxcd.io
  #         kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml

  #         FLUX_GIT_READ_ONLY=${{ steps.flux-git-read-only.outputs.result }}

  #         if [[ $FLUX_GIT_READ_ONLY = "false" ]]; then
  #           GIT_URL=git@github.com:${{ env.RELEASES_REPOSITORY }}
  #         elif [[ $FLUX_GIT_READ_ONLY = "true" ]]; then
  #           GIT_URL=https://github.com/${{ env.RELEASES_REPOSITORY }}
  #         fi

  #         helm upgrade flux fluxcd/flux \
  #           --set git.url=$GIT_URL \
  #           --set git.path="$CLUSTER_ENV" \
  #           --set git.label="flux-sync-$CLUSTER_ENV" \
  #           --set git.pollInterval="2m" \
  #           --set git.timeout="40s" \
  #           --set git.readonly=$FLUX_GIT_READ_ONLY \
  #           --set syncGarbageCollection.enabled=true \
  #           --namespace flux \
  #           --install --wait

  #         helm upgrade helm-operator fluxcd/helm-operator \
  #           --set git.ssh.secretName=flux-git-deploy \
  #           --set helm.versions=v3 \
  #           --set git.pollInterval="2m" \
  #           --set git.timeout="40s" \
  #           --namespace flux \
  #           --install --wait
  #       env:
  #         ENVIRONMENT_NAME: ${{ matrix.environment-name }}

  #     - id: get-public-key
  #       name: Get flux SSH deploy key
  #       if: steps.flux-git-read-only.outputs.result == 'false'
  #       run: |-
  #         fluxctl identity --k8s-fwd-ns flux | tee ~/flux.pub

  #     - id: add-key-to-github
  #       name: Add Flux deploy key to GitHub repository
  #       if: steps.flux-git-read-only.outputs.result == 'false'
  #       run: |-
  #         # find existing key IDs. save them to a file
  #         curl \
  #           -H"Authorization: token $API_TOKEN_GITHUB"\
  #           https://api.github.com/repos/${{ env.RELEASES_REPOSITORY }}/keys 2>/dev/null\
  #           | jq '.[] | select(.title | contains(env.CLUSTER_ENV)) | .id' > /tmp/key_ids

  #         # iterate through them and delete all existing deploy keys
  #         cat /tmp/key_ids | \
  #           while read _id; do
  #             echo "- delete  deploy key: $_id"
  #             curl \
  #               -X "DELETE"\
  #               -H"Authorization: token $API_TOKEN_GITHUB"\
  #               https://api.github.com/repos/${{ env.RELEASES_REPOSITORY }}/keys/$_id 2>/dev/null
  #           done

  #         # add the keyfile to github
  #         echo
  #         echo "+ flux deploy key:"
  #         echo -n ">> "
  #         {
  #           curl \
  #             -i\
  #             -H"Authorization: token $API_TOKEN_GITHUB"\
  #             --data @- https://api.github.com/repos/${{ env.RELEASES_REPOSITORY }}/keys << EOF
  #           {
  #             "title" : "Flux CI -- $CLUSTER_ENV -- $(date)",
  #             "key" : "$(cat ~/flux.pub)",
  #             "read_only" : false
  #           }
  #         EOF
  #         } 2>/dev/null | tee /tmp/create_key_result | head -1

  #         # check if key was created
  #         KEY_CREATED=$(grep 201 /tmp/create_key_result | wc -l | xargs)
  #         if [ $KEY_CREATED -eq 0 ]
  #         then
  #           echo
  #           echo Key creation failed. Full response shown below:
  #           cat /tmp/create_key_result
  #           false
  #         fi

  #     - id: deploy-xray-daemon
  #       name: Deploy AWS X-Ray daemon
  #       run: |-
  #         helm upgrade "aws-xray-daemon" infra/aws-xray-daemon \
  #           --namespace default \
  #           --set iamRole=arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/xray-daemon-role-$CLUSTER_ENV \
  #           --install --wait

  #     - id: install-ebs-csi-driver
  #       name: Install AWS EBS Container Storage Interface (CSI) drivers
  #       run: |-
  #         helm upgrade \
  #           aws-ebs-csi-driver https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/helm-chart-aws-ebs-csi-driver-2.6.4/aws-ebs-csi-driver-2.6.4.tgz \
  #           --namespace kube-system \
  #           --set enableVolumeScheduling=true \
  #           --set enableVolumeResizing=true \
  #           --set enableVolumeSnapshot=true \
  #           --install --wait \

  #     - id: deploy-read-only-group
  #       name: Deploy read-only permission definition for cluster
  #       run: |-
  #         helm upgrade "biomage-read-only-group" infra/biomage-read-only-group \
  #           --install --wait

  #     - id: deploy-state-machine-role
  #       name: Deploy AWS Step Function (state machine) roles
  #       uses: aws-actions/aws-cloudformation-github-deploy@v1
  #       with:
  #         parameter-overrides: "Environment=${{ matrix.environment-type }}"
  #         name: "biomage-state-machine-role-${{ matrix.environment-type }}"
  #         template: 'infra/cf-state-machine-role.yaml'
  #         capabilities: 'CAPABILITY_IAM,CAPABILITY_NAMED_IAM'
  #         no-fail-on-empty-changeset: "1"

  #     - id: remove-identitymappings
  #       name: Remove all previous identity mappings for IAM users
  #       run: |-
  #         eksctl get iamidentitymapping --cluster=biomage-$CLUSTER_ENV --output=json | \
  #         jq -r '.[] | select(.userarn != null) | .userarn' > /tmp/users_to_remove

  #         while IFS= read -r user
  #         do
  #           echo "Remove rights of $user"
  #           eksctl delete iamidentitymapping \
  #             --cluster=biomage-$CLUSTER_ENV \
  #             --arn $user \
  #             --all
  #         done < "/tmp/users_to_remove"

  #     # see https://eksctl.io/usage/iam-identity-mappings/
  #     - id: add-state-machine-role
  #       name: Grant rights to the state machine IAM role.
  #       run: |-
  #         eksctl create iamidentitymapping \
  #           --cluster=biomage-$CLUSTER_ENV \
  #           --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/state-machine-role-$CLUSTER_ENV \
  #           --group state-machine-runner-group \
  #           --username state-machine-runner

  #     # see https://eksctl.io/usage/iam-identity-mappings/
  #     # NOTE: after updating this step, make sure you apply the updates in other relevant Github Actions workflows
  #     - id: update-identitymapping-admin
  #       name: Add cluster admin rights to everyone on the admin list.
  #       run: |-
  #         echo "Reading cluster rights from file: infra/config/cluster/${{matrix.environment-name}}/cluster-admins-$CLUSTER_ENV"
  #         while IFS= read -r user
  #         do
  #           echo "Adding cluster admin rights to $user"
  #           eksctl create iamidentitymapping \
  #             --cluster=biomage-$CLUSTER_ENV \
  #             --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:user/$user \
  #             --group system:masters \
  #             --username $user
  #         done < "infra/config/cluster/${{matrix.environment-name}}/cluster-admins-$CLUSTER_ENV"

  # report-if-failed:
  #   name: Report if workflow failed
  #   runs-on: ubuntu-20.04
  #   needs: [setup, create-eks-cluster, configure-cluster]
  #   if: failure() && github.ref == 'refs/heads/master'
  #   steps:
  #     - id: send-to-slack
  #       name: Send failure notification to Slack on failure
  #       env:
  #         SLACK_BOT_TOKEN: ${{ secrets.WORKFLOW_STATUS_BOT_TOKEN }}
  #       uses: voxmedia/github-action-slack-notify-build@v1
  #       with:
  #         channel: workflow-failures
  #         status: FAILED
  #         color: danger
