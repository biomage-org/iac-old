name: Deploy Biomage infrastructure on AWS
on:
  workflow_dispatch:
    inputs:
      environment_name:
        type: choice
        description: Select the environment name to run the actions on
        options:
          - trv
          - alabs
          - Biomage
          - all
        default: all
      workflow_actions:
        type: choice
        description: Select actions to perform
        options:
          - create and configure cluster
          - configure cluster
          - deploy monitoring
        default: configure cluster
      environment_type:
        type: choice
        description: Select environment type
        options:
          - staging
          - production
          - staging and production
        default: staging

# this ensures that only one CI pipeline with the same key
#  can run at once in order to prevent undefined states
concurrency: cluster-update-mutex

permissions:
  id-token: write
  contents: read

# After set-environments and check-secrets jobs are finished:
#   "create and configure cluster" workflow_actions option runs all jobs.
#   "configure cluster" workflow_actions option runs only configure-cluster job
#   "deploy monitoring" workflow_actions option runs only deploy-monitoring job
jobs:
  set-environments:
    name: Set up environment name and environment type for action run
    runs-on: ubuntu-20.04
    outputs:
      env-type: ${{ steps.set-env-type.outputs.env-type }}
      env-name: ${{ steps.set-env-name.outputs.env-name }}
    steps:
      - id: set-env-type
        name: Set up environment type
        run: |-
          if [ "${ENVIRONMENT_TYPE}" = "staging" ]; then
            echo 'env-type=["staging"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_TYPE}" = "production" ]; then
            echo 'env-type=["production"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_TYPE}" = "staging and production" ]; then
            echo 'env-type=["staging", "production"]' >> $GITHUB_OUTPUT
          fi
        env:
          ENVIRONMENT_TYPE: ${{ github.event.inputs.environment_type }}
      - id: set-env-name
        name: Set up environment name
        run: |-
          if [ "${ENVIRONMENT_NAME}" = "all" ]; then
            echo 'env-name=["Biomage", "trv", "alabs"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "trv" ]; then
            echo 'env-name=["trv"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "alabs" ]; then
            echo 'env-name=["alabs"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "Biomage" ]; then
            echo 'env-name=["Biomage"]' >> $GITHUB_OUTPUT
          fi
        env:
          ENVIRONMENT_NAME: ${{ github.event.inputs.environment_name }}

  check-secrets:
    name: Check that sufficient secrets are specified for environment name
    runs-on: ubuntu-20.04
    needs: set-environments
    strategy:
      matrix:
        environment-name: ${{ fromJson(needs.set-environments.outputs.env-name) }}
    environment: ${{ matrix.environment-name }}
    steps:
      - id: check-secrets-for-environment
        name: Check if necessary secrets are installed.
        run: |-
          echo Checking if secrets are defined in the repository.
          if [ -z "${{ secrets.ACM_CERTIFICATE_ARN}}" ]
          then
            echo AWS certificate ARN is not defined.
            ERROR=true
          fi
          if [ -z "${{ secrets.AWS_ACCOUNT_ID }}" ]
          then
            echo AWS Account ID is not defined.
            ERROR=true
          fi
          if [ -z "${{ secrets.API_TOKEN_GITHUB }}" ]
          then
            echo GitHub deploy key access token is not defined.
            ERROR=true
          fi
          if [ -z "${{ secrets.PRIMARY_DOMAIN_NAME }}" ]
          then
            echo Secret PRIMARY_DOMAIN_NAME is not set in repository secrets. Make sure this secret exists in the repository secrets.
            ERROR=true
          fi
          if [ -z "${{ secrets.DOMAIN_NAME }}" ]
          then
            echo Secret DOMAIN_NAME is not set in repository secrets. Make sure this secret exists in the repository secrets.
            ERROR=true
          fi
          if [ -z "${{ secrets.DATADOG_API_KEY }}" ]
          then
            echo Secret DATADOG_API_KEY is not set in repository secrets. Make sure this secret exists in the repository secrets.
            ERROR=true
          fi
          if [ -z "${{ secrets.DATADOG_APP_KEY }}" ]
          then
            echo Secret DATADOG_APP_KEY is not set in repository secrets. Make sure this secret exists in the repository secrets.
            ERROR=true
          fi
          if [ ! -z "$ERROR" ]
          then
            echo
            echo This workflow requires some secrets to complete.
            echo Please make they are created by adding/rotating them manually.
            exit 1
          fi

  create-eks-cluster:
    name: Create EKS cluster
    runs-on: ubuntu-20.04
    needs: [set-environments, check-secrets]
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
    strategy:
      max-parallel: 1
      matrix:
        environment-type: ${{ fromJson(needs.set-environments.outputs.env-type) }}
        environment-name: ${{ fromJson(needs.set-environments.outputs.env-name) }}
        exclude:
          - environment-name: trv
            environment-type: staging
          - environment-name: alabs
            environment-type: staging
    environment: ${{ matrix.environment-name }}
    if: github.event.inputs.workflow_actions == 'create and configure cluster'
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/ci-iac-role
          role-duration-seconds: 4800
          aws-region: ${{ secrets.AWS_REGION }}

      - id: fill-metadata
        name: Add name and region to the eksctl file.
        run: |-
          export CLUSTER_NAME="biomage-$CLUSTER_ENV"

          yq -i '
            .metadata.name = strenv(CLUSTER_NAME) |
            .metadata.region = strenv(AWS_REGION)
          ' infra/config/cluster/cluster-template.yaml

          export CUSTOM_CLUSTER_CONFIG="infra/config/cluster/${{ matrix.environment-name }}/custom-cluster-config.yaml"
          yq eval-all '. as $item ireduce ({}; . *d $item)' infra/config/cluster/cluster-template.yaml ${CUSTOM_CLUSTER_CONFIG} > /tmp/cluster-$CLUSTER_ENV.yaml
          cat /tmp/cluster-$CLUSTER_ENV.yaml
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}

      - id: install-eksctl
        name: Install eksctl
        run: |-
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - id: create-clusters
        name: Attempt to create clusters from spec.
        # this job will always pass, irrespective of whether creation was successful or not.
        # this is because the cluster may already exist. we will check for this condition
        # on failure in the next step
        continue-on-error: true
        run: |-
          exec &> >(tee /tmp/eksctl-$CLUSTER_ENV.log)

          eksctl create cluster -f /tmp/cluster-$CLUSTER_ENV.yaml

          echo "outcome=created" >> $GITHUB_OUTPUT

      - id: check-for-failure
        name: Check for reason of failure if cluster creation failed.
        if: steps.create-clusters.outcome == 'failure'
        run: |-
          # Check if failure was caused by an already exists exception.
          # If not, the job should fail.
          ALREADY_EXISTS=$(grep AlreadyExistsException /tmp/eksctl-$CLUSTER_ENV.log | wc -l | xargs)
          if [ $ALREADY_EXISTS -ne 1 ]
          then
            echo Step failed for reason other than stack already existing.
            echo Job failing...
            echo "reason=error" >> $GITHUB_OUTPUT
            false
          fi

          echo Cluster already exists.
          echo "reason=already-exists" >> $GITHUB_OUTPUT

      - id: update-addons-for-cluster
        name: Attempt to create addons for cluster.
        continue-on-error: true
        run: |-
          exec &> >(tee /tmp/eksctl-$CLUSTER_ENV.log)

          eksctl create addon -f /tmp/cluster-$CLUSTER_ENV.yaml

      - id: update-nodegroup
        name: Attempt to update node groups for existing cluster.
        if: steps.create-clusters.outcome == 'failure' && steps.check-for-failure.outputs.reason == 'already-exists'
        run: |-
          eksctl create nodegroup --config-file=/tmp/cluster-$CLUSTER_ENV.yaml
          eksctl delete nodegroup --config-file /tmp/cluster-$CLUSTER_ENV.yaml --only-missing --approve

      # note: iam service accounts should really be created from within the helm chart as seen here:
      # https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html
      - id: update-serviceaccounts
        name: Attempt to update IAM service accounts for existing cluster.
        if: steps.create-clusters.outcome == 'failure' && steps.check-for-failure.outputs.reason == 'already-exists'
        run: |-
          eksctl utils associate-iam-oidc-provider --config-file=/tmp/cluster-$CLUSTER_ENV.yaml --approve
          eksctl create iamserviceaccount --config-file=/tmp/cluster-$CLUSTER_ENV.yaml
          eksctl delete iamserviceaccount --config-file=/tmp/cluster-$CLUSTER_ENV.yaml --only-missing --approve

  configure-cluster:
    name: Configure Kubernetes resources on the EKS cluster
    runs-on: ubuntu-20.04
    needs: [set-environments, check-secrets, create-eks-cluster]
    if: always() && github.event.inputs.workflow_actions != 'deploy monitoring' && (needs.create-eks-cluster.result == 'success' || needs.create-eks-cluster.result == 'skipped')
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
      API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
    strategy:
      max-parallel: 1
      matrix:
        environment-type: ${{ fromJson(needs.set-environments.outputs.env-type) }}
        environment-name: ${{ fromJson(needs.set-environments.outputs.env-name) }}
        exclude:
          - environment-name: trv
            environment-type: staging
          - environment-name: alabs
            environment-type: staging
    environment: ${{ matrix.environment-name }}
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/ci-iac-role
          role-duration-seconds: 4800
          aws-region: ${{ secrets.AWS_REGION }}

      - id: add-kubeconfig
        name: Add k8s config file for existing cluster.
        run: |-
          aws eks update-kubeconfig --name biomage-$CLUSTER_ENV

      - id: deploy-metrics-server
        name: Deploy k8s metrics server
        run: |-
          kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

      - id: install-helm
        name: Install Helm
        run: |-
          sudo snap install helm --classic

      - id: install-eksctl
        name: Install eksctl
        run: |-
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - id: deploy-load-balancer-role
        name: Deploy permissions for AWS load balancer controller
        run: |-
          curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.0/docs/install/iam_policy.json
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy-$CLUSTER_ENV \
            --policy-document file://iam-policy.json || true
          eksctl create iamserviceaccount \
            --cluster=biomage-$CLUSTER_ENV \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:policy/AWSLoadBalancerControllerIAMPolicy-$CLUSTER_ENV \
            --role-name eksctl-$CLUSTER_ENV-load-balancer-controller-role \
            --override-existing-serviceaccounts \
            --approve

      # we need to retry this due to an active issue with the AWS Load Balancer Controller
      # where there are intermittent failures that are only fixable by retrying
      # see issue at https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/2071
      - id: install-lbc
        name: Deploy AWS Load Balancer Controller
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 600
          max_attempts: 20
          retry_on: error
          on_retry_command: sleep $(shuf -i 5-15 -n 1)
          command: |-
            helm repo add eks https://aws.github.io/eks-charts
            kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
            helm repo update
            helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \
              --namespace kube-system \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --set clusterName=biomage-$CLUSTER_ENV \
              --install --wait

      - id: platform-public-facing
        name: Get config for whether platform should be public facing
        uses: mikefarah/yq@master
        with:
          cmd: yq '.[env(ENVIRONMENT_NAME)].publicFacing' 'infra/config/github-environments-config.yaml'
        env:
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: install-elb-503-subscription-endpoint
        name: Install ELB 503 subscription endpoint
        run: |-
            echo "value of publicFacing: $PUBLIC_FACING"

            # Check that publicFacing is set to true or false
            if [ "$PUBLIC_FACING" != "true" ] && [ "$PUBLIC_FACING" != "false" ]; then
              echo "value of publicFacing in infra/config/github-environments-config.yaml is not set to true or false"
              exit 1
            fi

            # this is needed so SNS does not stop trying to subscribe to not-yet-deployed
            # API staging environments because their endpoints are not yet available.
            helm upgrade aws-elb-503-subscription-endpoint infra/aws-elb-503-subscription-endpoint \
              --namespace default \
              --set clusterEnv=$CLUSTER_ENV \
              --set acmCertificate=${{ secrets.ACM_CERTIFICATE_ARN }} \
              --set-string publicFacing="$PUBLIC_FACING" \
              --install --wait
        env:
          PUBLIC_FACING: ${{ steps.platform-public-facing.outputs.result }}

      - id: deploy-env-loadbalancer
        name: Deploy AWS Application Load Balancer for environment
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }},PublicFacing=${{ steps.platform-public-facing.outputs.result }}"
          name: "biomage-k8s-alb-${{ matrix.environment-type }}"
          template: 'infra/cf-loadbalancer.yaml'
          no-fail-on-empty-changeset: "1"

      - id: setup-domain
        name: Compile environment-specific domain name
        run: |-
          if [ "${{ matrix.environment-type }}" = "production" ]; then
            PRIMARY_DOMAIN_NAME="${{ secrets.PRIMARY_DOMAIN_NAME }}"
            DOMAIN_NAME="${{ secrets.DOMAIN_NAME }}"
          fi
          if [ "${{ matrix.environment-type }}" = "staging" ]; then
            PRIMARY_DOMAIN_NAME="${{ secrets.PRIMARY_DOMAIN_NAME }}"
            DOMAIN_NAME="${{ secrets.DOMAIN_NAME_STAGING }}"
          fi
          echo "primary-domain-name=$PRIMARY_DOMAIN_NAME" >> $GITHUB_OUTPUT
          echo "domain-name=$DOMAIN_NAME" >> $GITHUB_OUTPUT

      # Existing Route53 records have to be deleted so that we can deploy ELB-associated Route53 records
      # Otherwise CloudFormation will fail saying the Route53 records exists
      - id: check-for-existing-route53-records
        name: Check for existing Route53 records
        run: |-
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name | jq ".HostedZones[0].Id" | sed 's/\/hostedzone\///g')
            RECORD_SETS=$(aws route53 list-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID | jq ".ResourceRecordSets[] | select( .Name | match(\"^$DOMAIN_NAME\"))"

            echo "hosted-zone-id=$HOSTED_ZONE_ID" >> $GITHUB_OUTPUT
            echo "existing-route53-records=$RECORD_SETS" >> $GITHUB_OUTPUT

      - id: delete-existing-route53-records
        name: Delete existing Route53 records, if any
        if: ${{ env.EXISTING_RECORDS != '' }}
        env:
            DOMAIN_NAME: ${{ secrets.DOMAIN_NAME }}
            PRIMARY_DOMAIN_NAME: ${{ secrets.PRIMARY_DOMAIN_NAME }}
            HOSTED_ZONE_ID: ${{ steps.check-for-existing-route53-records.outputs.hosted-zone-id }}
            EXISTING_RECORDS: ${{ steps.check-for-existing-route53-records.outputs.existing-route53-records }}
        run: |-
            sed "s/DOMAIN_NAME/$DOMAIN_NAME/g" infra/route53/route53-changes.json >>  /temp/delete-route53-records.json

            aws route53 change-resource-record-sets \
                --hosted-zone-id $HOSTED_ZONE_ID \
                --change-batch file:///tmp/delete-route53-records.json

      # This is commented out because running Route53 for trv is still causing an error.
      # A ticket to address this issue has been made: BIOMAGE-2314. Refer to the ticket for more information.
      - id: deploy-route53
        name: Deploy Route 53 DNS records to ELB
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }},DNSName=${{ steps.deploy-env-loadbalancer.outputs.DNSName }},HostedZoneId=${{ steps.deploy-env-loadbalancer.outputs.CanonicalHostedZoneID }},PrimaryDomainName=${{ steps.setup-domain.outputs.primary-domain-name }},DomainName=${{ steps.setup-domain.outputs.domain-name }}"
          name: "biomage-alb-route53-${{ matrix.environment-type }}"
          template: 'infra/cf-route53.yaml'
          no-fail-on-empty-changeset: "1"

      - id: deploy-xray-daemon
        name: Deploy AWS X-Ray daemon
        run: |-
          helm upgrade "aws-xray-daemon" infra/aws-xray-daemon \
            --namespace default \
            --set iamRole=arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/xray-daemon-role-$CLUSTER_ENV \
            --install --wait

      - id: install-ebs-csi-driver
        name: Install AWS EBS Container Storage Interface (CSI) drivers
        run: |-
          helm upgrade \
            aws-ebs-csi-driver https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/helm-chart-aws-ebs-csi-driver-2.17.2/aws-ebs-csi-driver-2.17.2.tgz \
            --namespace kube-system \
            --set enableVolumeScheduling=true \
            --set enableVolumeResizing=true \
            --set enableVolumeSnapshot=true \
            --install --wait

      - id: deploy-read-only-group
        name: Deploy read-only permission definition for cluster
        run: |-
          helm upgrade "biomage-read-only-group" infra/biomage-read-only-group \
            --install --wait

      - id: deploy-state-machine-role
        name: Deploy AWS Step Function (state machine) roles
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }}"
          name: "biomage-state-machine-role-${{ matrix.environment-type }}"
          template: 'infra/cf-state-machine-role.yaml'
          capabilities: 'CAPABILITY_IAM,CAPABILITY_NAMED_IAM'
          no-fail-on-empty-changeset: "1"

      - id: remove-identitymappings
        name: Remove all previous identity mappings for IAM users
        run: |-
          eksctl get iamidentitymapping --cluster=biomage-$CLUSTER_ENV --output=json | \
          jq -r '.[] | select(.userarn != null) | .userarn' > /tmp/users_to_remove
          while IFS= read -r user
          do
            echo "Remove rights of $user"
            eksctl delete iamidentitymapping \
              --cluster=biomage-$CLUSTER_ENV \
              --arn $user \
              --all
          done < "/tmp/users_to_remove"

      # see https://eksctl.io/usage/iam-identity-mappings/
      # Grant login rights to ci-iac-role
      - id: add-ci-iac-oidc-cluster-role
        name: Allow the OIDC role to log in to our cluster.
        run: |-
          eksctl create iamidentitymapping \
            --cluster=biomage-$CLUSTER_ENV \
            --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/ci-iac-role \
            --group system:masters \
            --username ci-iac-role

      # SSO access to cluster is only added if accessing AWS and cluster using SSO
      - id: allow-sso-roles-to-access-cluster
        env:
          SSO_ROLE: ${{ secrets.SSO_ROLE }}
        if: ${{ env.SSO_ROLE != '' }}
        name: Allow SSO role to log into the cluster
        run: |-
          eksctl create iamidentitymapping \
            --cluster biomage-$CLUSTER_ENV \
            --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/${{ env.SSO_ROLE }} \
            --username sso-cluster-admin \
            --no-duplicate-arns \
            --group system:masters

      - id: add-state-machine-cluster-role
        name: Grant rights to the state machine IAM role.
        run: |-
          eksctl create iamidentitymapping \
            --cluster=biomage-$CLUSTER_ENV \
            --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/state-machine-role-$CLUSTER_ENV \
            --group state-machine-runner-group \
            --username state-machine-runner

      # NOTE: after updating this step, make sure you apply the updates in other relevant Github Actions workflows
      - id: update-identitymapping-admin
        name: Add cluster admin rights to everyone on the admin list.
        run: |-
          echo "Reading cluster rights from file: infra/config/cluster/${{matrix.environment-name}}/cluster-admins-$CLUSTER_ENV"
          while IFS= read -r user
          do
            echo "Adding cluster admin rights to $user"
            eksctl create iamidentitymapping \
              --cluster=biomage-$CLUSTER_ENV \
              --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:user/$user \
              --group system:masters \
              --username $user
          done < "infra/config/cluster/${{matrix.environment-name}}/cluster-admins-$CLUSTER_ENV"

      ###
      ### INSTALL AND CONFIGURE FLUX V2 ###
      ###
      - id: using-self-signed-certificate
        name: Get config for whether deployment is using self-signed certificate
        uses: mikefarah/yq@master
        with:
          cmd: yq '.[env(ENVIRONMENT_NAME)].selfSignedCertificate' 'infra/config/github-environments-config.yaml'
        env:
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: fill-account-specific-metadata
        name: Fill in account specific metadata in ConfigMap
        run: |-
          export DATADOG_API_KEY="${{ secrets.DATADOG_API_KEY }}"
          export DATADOG_APP_KEY="${{ secrets.DATADOG_APP_KEY }}"

          yq -i '
            .myAccount.domainName = strenv(DOMAIN_NAME) |
            .myAccount.region = strenv(AWS_REGION) |
            .myAccount.accountId = strenv(AWS_ACCOUNT_ID) |
            .myAccount.publicFacing = strenv(PUBLIC_FACING) |
            .myAccount.acmCertificate = strenv(ACM_CERTIFICATE_ARN) |
            .myAccount.selfSignedCertificate = strenv(SELF_SIGNED_CERTIFICATE) |
            .myAccount.datadogApiKey = strenv(DATADOG_API_KEY) |
            .myAccount.datadogAppKey = strenv(DATADOG_APP_KEY)
          ' infra/config/account-config.yaml

          cat infra/config/account-config.yaml
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}
          DOMAIN_NAME: ${{ steps.setup-domain.outputs.domain-name }}
          ACM_CERTIFICATE_ARN: ${{ secrets.ACM_CERTIFICATE_ARN }}
          PUBLIC_FACING: ${{ steps.platform-public-facing.outputs.result }}
          SELF_SIGNED_CERTIFICATE: ${{ steps.using-self-signed-certificate.outputs.result }}

      - id: create-flux-namespace
        name: Attempt to create flux namespace
        continue-on-error: true
        run: |-
          kubectl create namespace flux-system

      - id: create-account-information-configmap
        name: Create a configmap containing AWS account specific details
        continue-on-error: false
        run: |-
          kubectl create configmap account-config --from-file=infra/config/account-config.yaml -n flux-system -o yaml --dry-run | kubectl apply -f -

      - id: install-flux-v2
        name: Install flux CLI version 0.41.0
        run: |-
          curl -s https://fluxcd.io/install.sh | sudo FLUX_VERSION=0.41.0 bash

      - id: delete-old-flux-github-deploy-key
        name: Attempt to delete previous github flux deploy key
        continue-on-error: true
        run: |-
          kubectl -n flux-system delete secret flux-system

      - id: install-flux
        name: Install Flux to EKS cluster
        run: |-

          # Refer to https://github.com/fluxcd/flux2/releases
          FLUX_VERSION=v0.41.0
          FLUX_REPO=releases
          FLUX_PATH=deployments/$ENVIRONMENT_NAME-$CLUSTER_ENV
          REPO_FULL_PATH=$GITHUB_REPOSITORY_OWNER/$FLUX_REPO

          echo "flux-full-repo=$(echo $REPO_FULL_PATH)" >> $GITHUB_ENV
          echo "flux-path=$(echo $FLUX_PATH)" >> $GITHUB_ENV

          args=(
            --version $FLUX_VERSION
            --owner $GITHUB_REPOSITORY_OWNER
            --repository $FLUX_REPO
            --branch master
            --path $FLUX_PATH
            --timeout 40s
            –-interval 2m
            --components-extra=image-reflector-controller,image-automation-controller
            --namespace flux-system
            --cluster arn:aws:eks:$AWS_REGION:$AWS_ACCOUNT_ID:cluster/biomage-$CLUSTER_ENV
            --context arn:aws:eks:$AWS_REGION:$AWS_ACCOUNT_ID:cluster/biomage-$CLUSTER_ENV
          )

          if [ "${{ matrix.environment-type }}" = "staging" ]
          then
            echo Flux will be deployed in staging with read and write permissions
            args+=(--read-write-key)
          elif [ "${{ matrix.environment-type }}" = "production" ]
          then
            echo Flux will be deployed in production with read-only permissions
          fi

          flux bootstrap github "${args[@]}"

        env:
          GITHUB_TOKEN: ${{ secrets.API_TOKEN_GITHUB }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: fill-in-sync-yaml
        name: Create the sync.yaml file that contains the Kustomization to sync the cluster
        run: |-
          export SPEC_PATH="./$CLUSTER_ENV"
          yq -i '
            .spec.path = strenv(SPEC_PATH)
          ' infra/flux/sync.yaml

          cat infra/flux/sync.yaml

      - id: push-sync-yaml
        name: Push the sync.yaml file that was filled in during the previous step
        uses: dmnemec/copy_file_to_another_repo_action@v1.0.4
        env:
          API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
        with:
          source_file: infra/flux/sync.yaml
          destination_repo: ${{ env.flux-full-repo }}
          destination_folder: ${{ env.flux-path }}
          user_email: ci@biomage.net
          user_name: 'Biomage CI/CD'

      - id: fill-kustomization-template
        name: Fill in Kustomization template
        run: |-
          cat infra/flux/kustomization-template.yaml \
            | sed "s/AWS_ACCOUNT_ID/$AWS_ACCOUNT_ID/g" \
            | sed "s/CLUSTER_ENV/$CLUSTER_ENV/g" \
            > infra/flux/kustomization.yaml

          cat infra/flux/kustomization.yaml
        env:
          AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}

      - id: push-kustomization-yaml
        name: Push the kustomization.yaml file to apply our custom config
        uses: dmnemec/copy_file_to_another_repo_action@v1.0.4
        env:
          API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
        with:
          source_file: infra/flux/kustomization.yaml
          destination_repo: ${{ env.flux-full-repo }}
          destination_folder: ${{ env.flux-path }}/flux-system
          user_email: ci@biomage.net
          user_name: 'Biomage CI/CD'

      - id: install-kubernetes-reflector
        name: Install kubernetes reflector
        run: |-
          helm repo add emberstack https://emberstack.github.io/helm-charts
          helm repo update
          helm upgrade --install reflector emberstack/reflector --namespace flux-system

      - id: add-account-config-configmap-annotations
        name: Add annotations to account-config configmap
        run: |-
          kubectl annotate configmap account-config \
            --overwrite \
            --namespace flux-system \
            reflector.v1.k8s.emberstack.com/reflection-allowed="true" \
            reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces="ui-.*,api-.*,pipeline-.*,worker-.*" \
            reflector.v1.k8s.emberstack.com/reflection-auto-enabled="true"
      ###
      ### END OF INSTALL AND CONFIGURE FLUX V2 ###
      ###

  deploy-monitoring:
    name: Setup logging and monitoring
    runs-on: ubuntu-20.04
    needs: [set-environments, check-secrets, create-eks-cluster, configure-cluster]
    if: always() && github.event.inputs.workflow_actions != 'configure cluster' && (needs.create-eks-cluster.result == 'success' || needs.create-eks-cluster.result == 'skipped' && (needs.configure-cluster.result == 'success' || needs.configure-cluster.result == 'skipped'))
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
      API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
    strategy:
      matrix:
        environment-type: ${{ fromJson(needs.set-environments.outputs.env-type) }}
        environment-name: ${{ fromJson(needs.set-environments.outputs.env-name) }}
        exclude:
          - environment-name: trv
            environment-type: staging
          - environment-name: alabs
            environment-type: staging
    environment: ${{ matrix.environment-name }}
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/ci-iac-role
          role-duration-seconds: 4800
          aws-region: ${{ secrets.AWS_REGION }}

      - id: add-kubeconfig
        name: Add k8s config file for existing cluster.
        run: |-
          aws eks update-kubeconfig --name biomage-$CLUSTER_ENV

      - id: install-eksctl
        name: Install eksctl
        run: |-
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - id: setup-cluster-cloudwatch-logging-policy
        name: Setup permissions required for cluster to log to Cloudwatch
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }}"
          name: "cluster-cloudwatch-logging-policy-${{ matrix.environment-type }}"
          template: 'infra/cluster-logging/cf-cluster-log-cloudwatch-policy.yaml'
          no-fail-on-empty-changeset: "1"
          capabilities: "CAPABILITY_IAM,CAPABILITY_NAMED_IAM"

      # Setting up log forwarding for pods hosted in EC2 nodes
      - id: create-fluent-bit-namespace
        name: Create namespace for node FluentBit deployment
        run: kubectl apply -f infra/cluster-logging/node-fluentbit-namespace.yaml

      - id: create-service-account-for-node-fluent-bit
        name: Create service account for node FluentBit
        env:
          LOGGING_POLICY_ARN: ${{ steps.setup-cluster-cloudwatch-logging-policy.outputs.PolicyARN }}
        run: |-
          eksctl create iamserviceaccount \
            --name fluent-bit \
            --namespace node-logging \
            --cluster biomage-$CLUSTER_ENV \
            --role-name irsa-fluent-bit-$CLUSTER_ENV \
            --attach-policy-arn $LOGGING_POLICY_ARN \
            --override-existing-serviceaccounts \
            --approve

      - id: deploy-node-fluent-bit
        name: Deploy FluentBit for EC2 nodes
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          # FluentBit configuration is determined in infra/cluster-logging/node-fluentbit-config.yaml, specifically under [INPUT] > Path
          # We do not want to log everything for costs/security concerns

          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_CLUSTER_ENV\", \"$CLUSTER_ENV\")" infra/cluster-logging/node-fluentbit-config.yaml
          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_AWS_REGION\", \"$AWS_REGION\")" infra/cluster-logging/node-fluentbit-config.yaml

          kubectl apply -f infra/cluster-logging/node-fluentbit-config.yaml

      # Setting up log forwarding for pods hosted on Fargate nodes
      - id: attach-pod-execution-role-name
        name: Attach logging policy to pod execution role
        env:
          LOGGING_POLICY_ARN: ${{ steps.setup-cluster-cloudwatch-logging-policy.outputs.PolicyARN }}
        run: |-
          # Pods launched in the same cluster has the same pod execution role, as pod execution role scope is cluster-wide.
          # See https://eksctl.io/usage/fargate-support/#creating-a-cluster-with-fargate-support
          # Getting fargate-profile of pipeline or worker in the same cluster gets the same pod execution role.

          POD_EXEC_ROLE_NAME=$(aws eks describe-fargate-profile \
            --cluster-name biomage-$CLUSTER_ENV \
            --fargate-profile-name pipeline-default | jq -r '.fargateProfile.podExecutionRoleArn' | awk -F"/" '{print (NF>1)? $NF : ""}' )

          aws iam attach-role-policy --role-name $POD_EXEC_ROLE_NAME --policy-arn $LOGGING_POLICY_ARN

      - id: deploy-fargate-fluent-bit
        name: Deploy FluentBit config for Fargate pods
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |-
          # FluentBit configuration is determined in infra/cluster-logging/fargate-fluentbit-config.yaml
          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_CLUSTER_ENV\", \"$CLUSTER_ENV\")" infra/cluster-logging/fargate-fluentbit-config.yaml
          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_AWS_REGION\", \"$AWS_REGION\")" infra/cluster-logging/fargate-fluentbit-config.yaml

          kubectl apply -f infra/cluster-logging/fargate-fluentbit-config.yaml

      # Setting up Datadog to watch pod metrics for pods hosted on EC2 and Fargate nodes
      - id: setup-datadog-cluster-agent
        name: Setup Datadog cluster agent
        run: |-
          helm repo add datadog https://helm.datadoghq.com
          helm repo update
          helm upgrade datadog-agent datadog/datadog \
            -f infra/datadog/cluster-agent-values.yaml \
            --set datadog.apiKey=${{ secrets.DATADOG_API_KEY }} \
            --set datadog.clusterName=biomage-$CLUSTER_ENV \
            --install

      - id: setup-datadog-sidecar-permissions
        name: Setup Datadog sidecar permissions
        run: |-
          kubectl apply -f infra/datadog/datadog-sidecar-rbac.yaml

  report-if-failed:
    name: Report if workflow failed
    runs-on: ubuntu-20.04
    needs: [set-environments, check-secrets, create-eks-cluster, configure-cluster, deploy-monitoring]
    if: failure() && github.ref == 'refs/heads/master'
    steps:
      - id: send-to-slack
        name: Send failure notification to Slack on failure
        env:
          SLACK_BOT_TOKEN: ${{ secrets.WORKFLOW_STATUS_BOT_TOKEN }}
        uses: voxmedia/github-action-slack-notify-build@v1
        with:
          channel: workflow-failures
          status: FAILED
          color: danger
